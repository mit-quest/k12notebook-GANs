{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training DCGAN\n",
    "**Adapted from Alex Andonian's notebook of Gan Training**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial will give a brief introduction to training Generative Adversarial Networks (GANs), a rapidly improving field. GANs are still fairly young, first being developed on May 26, 2014. The rapid success of GANs' in image synthesis is particularly impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to GANs\n",
    "\n",
    "A **GAN**, Generative Adversarial Network, is a type of **machine learning** that uses a strategy involving two competing (adversarial) networks. The objective of the generator is to learn how to generate realistic looking images to trick the second **discriminator network** into thinking it is a real image. In the beginning an unlearned generator will produce random outputs, but as the network continues to run the accuracy of its generated images will improve and look more similar to the original dataset. Generally the discriminator network is pretrained on the dataset, however, it is also trained overtime.  \n",
    "\n",
    "Each time the generator generates new images, the discriminator 'gives feedback' on where the generator went wrong and provides confirmation about the areas the generator is doing well to teach it how to make better outputs. \n",
    "\n",
    "The generator network is training to generate an image near indistinguishable to a true image to \"trick\" the discriminator, while the discriminator is simultaneously training to increase its accuracy of distinguishing a generated image from a real image of the dataset. This competition allows the networks to continuously improve their accuracy with each run of the system.\n",
    "\n",
    "##### What does an image look like to a computer and how does it learn things about an image?\n",
    "To understand how GANs work, we first have to understand how an image is interpreted by a computer. To a computer, an image is a series of numbers. All pixels of the image are given a number and listed according to its dimensions. This list of numbers informs the computer about the individual pixel's position and color. Training a GAN teaches the generator to understand patterns of numbers in an image and it stores this information in the weights and connections between neurons.\n",
    "\n",
    "#### How Does A GAN Work?\n",
    "\n",
    "The GAN generator is a function z->x that transforms random z to realistic images x. Z is random input noise that is in the same dimension that our generated x image. The diagram below shows the general flow of the system.\n",
    "\n",
    "In this diagram:\n",
    "* Z - The random noise as input\n",
    "* X - The real images fed into the discriminator\n",
    "* X* - Generated images fed into the discriminator\n",
    "\n",
    "\n",
    "![gan.png](assets/gan-diagram.png) \n",
    "First the generator is fed random noise, Z. It then takes this input Z and runs it through the generator. Different neurons are activated by the input data and/or the activation of other neurons. These neurons change the image pixels that it controls based on its activation.\n",
    "\n",
    "The output of the generator, X*, is then merged with the real image dataset, X, which is passed to the discriminator.\n",
    "\n",
    "The discriminator attempts to separate the images back into real and generated images. The percentage of images that the discriminator classifies incorrectly is considered the **classification error**. The worst possible classification error is 50% because any higher error could have the classifications switched to get a lower classification error. \n",
    "\n",
    "* A high classification error means that the generator has created real enough looking images to trick the discriminator\n",
    "* A low classification error means that the discriminator could easily tell the real images from the fake ones\n",
    "\n",
    "\n",
    "After the discriminator has classified the images, that data is run back through the system through backpropagation to help adjust weights in both the generator and discriminator neural networks.\n",
    "\n",
    "### Examples of the different types of GANs:\n",
    "\n",
    "##### [Style GANs ](https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431)\n",
    "Style GANs are able to control the style of generated images at different levels of detail:\n",
    "\n",
    "This first example is a style GAN used to merge two people together to create a new \"person\" combining features from each:\n",
    "[![style_gan.gif](assets/style_gan.jpg)](https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431)\n",
    "\n",
    "This example converts an image into a painting of the style of an artist:\n",
    "[![style_gan2.gif](assets/style_gan2.jpeg)](https://deepart.io/)\n",
    "\n",
    "##### [Progressive GANs](https://towardsdatascience.com/progressively-growing-gans-9cb795caebee)\n",
    "Progans are a type of GAN in which images are trained at low resolution first and then are gradually trained to a higher resolution adding layers as the resolution increases.\n",
    "[![progan.gif](assets/progan.gif)](https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2)\n",
    "\n",
    "##### [BIGGANs](https://medium.com/syncedreview/biggan-a-new-state-of-the-art-in-image-synthesis-cf2ec5694024)\n",
    "The goal of BIGGANs is to successfully generate high resolution images at a large scale using a lot of iterations and model parameters. The result is the generation of both high-resolution (large) and high-quality (high-fidelity) images.\n",
    "\n",
    "[![biggan.png](assets/biggan.png)](https://medium.com/syncedreview/biggan-a-new-state-of-the-art-in-image-synthesis-cf2ec5694024)\n",
    "\n",
    "\n",
    "Here's a timeline of types of GANs that have developed since 2014:\n",
    "![timeline1.png](assets/timeline.png)\n",
    "![timeline2.png](assets/timeline1.png)\n",
    "\n",
    "### What we will be using\n",
    "In this notebook we will be training a Deep Convolutional GANs or [**DCGANs**](https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8), which set the stage for the success we are seeing today. In brief, DCGANs are a type of [convolutional network](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) that has a multi-layered architecture. These layers expand and increase in dimension as the system develops to process and retain more information about the dataset. DCGANs were developed in 2015, these were one of the first major improvements from the original GAN. DCGANs are still highly relevant and foundational to GANS. \n",
    "\n",
    "### Difficulties in training a GAN\n",
    "GANs can end up being [very difficult to train](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b) because of the finicky nature of having two networks compete against each other. Here are some of the errors that can occur while training.\n",
    "#### Non-converging Solutions\n",
    "If the training function does not find an optimal solution during training then the GAN is considered non-converging. A model will typically finish training at a Nash equilibrium. Since both sides want to beat each other, a Nash equilibrium occurs when one player will not change its action regardless of what the opponent may do. \n",
    "\n",
    "Think of a non-converging case as if we were competing two networks in a game of rock paper scissors. Because all options are perfectly balanced there are no optimal solutions so any strategy that one network comes up with will be countered with a counter strategy from the other. This new strategy will then be countered and so on creating a non-coptimizing cycle.\n",
    "\n",
    "We can also consider an example where two players A and B control the value of x and y respectively. In the diagram below Player A wants to maximize the value xy while B wants to minimize it.\n",
    "\n",
    "[![non-converging.png](assets/non_converging.png)](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
    "\n",
    "This GAN will never converge on a solution because any value of a player can be counteracted by negating the sign of the other player. Therefore x and y will be stuck in a continuous loop of positive and negative values forever.\n",
    "\n",
    "Therefore when designing GAN's we must be careful to determine if the model is non-converging otherwise we could waste time and money trying to train the GAN.\n",
    "\n",
    "#### Vanishing gradients\n",
    "\n",
    "Another problem with training GANs is that it’s easy for one network to overpower another inhibiting the training of both networks. This means that one of the networks isn’t receiving any positive feedback and the other network does not have any negative feedback. Thus neither network knows how to improve.\n",
    "\n",
    "This is because networks become better by learning from both their successes and failures. They use their outcome data to shift the network towards tendencies that most resemble the successes. This is a process called gradient descent as shown in the picture below. The function is attempting to minimize the error or cost on a training set.\n",
    "\n",
    "[![gradient.png](assets/Gradient_descent.png)](https://hackernoon.com/gradient-descent-aynk-7cbe95a778da)\n",
    "\n",
    "By adjusting weights of neurons in our network we are able to decrease the overall cost of the function along the gradient in the direction of less error.\n",
    "\n",
    "However, when one network (either generator or discriminator) overpowers the other, this function is more representative of a horizontal line. Therefore our networks do not know which way to adjust weights in order to minimize the cost. This is the problem of vanishing gradients and, in most cases, it occurs because the descrimator overpowers the generator because it trains faster.\n",
    "\n",
    "Therefore one of our goals in this notebook is to monitor the training of both networks so that they can improve simultaneously.\n",
    "\n",
    "#### Mode Collapse\n",
    "\n",
    "One of the most common errors in training a GAN is mode collapse. This occurs when the generator starts producing nearly identical images.\n",
    "\n",
    "Recall that the goal of our generator is to produce images that create the most classification error. If in our training we have stopped training the discriminator to let the generator improve compared to the discriminator (for example to fight against discriminator overpowering) it will begin to converge on producing an image that causes the most classification error.\n",
    "\n",
    "In an extreme example case the generator converges on one image (let's call it \"A\") making its generation completely independent of the random input Z into the generator.\n",
    "\n",
    "Now when we start training the discriminator again it easily determines that any image \"A\" is a generated image therefore the classification error is now 0%. The discriminator has now become specialized and only recognizes the one image, \"A\", as a generated image. Now the generator can change the image a tiny bit from \"A\" to completely throw off the discriminator and make the classification error 50%. Again the discriminator finds this image and now the process repeats.\n",
    "\n",
    "This destroys the functionality of both the generator and discriminator, and the generator no longer produces new images based on the random input.\n",
    "\n",
    "An example of partial mode collapse is shown below. Images with the same color underline are very similar.\n",
    "[![modecollapse.png](assets/mode-collapse.png)](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
    "\n",
    "#### What we can do\n",
    "\n",
    "We will be using various statistics in order to detect these problems in our GAN as shown later in this notebook.\n",
    "\n",
    "#### Now let's take a look at how to set up our GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "First, let's import the all of the python packages we will use throughout the tutorial. In addition to standard import and PyTorch, we provide a small package called ganocracy, which aggregates a host of useful GAN specific functions and utilities in one place for everything needed to get started with GANs.\n",
    "Almost all code shown in this notebook can be found inside the ganocracy package. We house reference implementations outside of the notebook so that bits and pieces can be conveniently incorporated into other projects, (without having to copy and paste from this notebook).\n",
    "Note: One major disadvantage of GAN’s is it is very difficult to “take a look inside” and make smart adjustments. We attempt to do this a little later in the GANdissect notebook.\n",
    " \n",
    "Below, we configure how the notebook will run and fill it with sensible defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard imports to make use of our system and matrix creation/manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is the library that we are going to be using to enable our machine learning. It handles a lot of the mathematical training of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GANOCRACY LIB\n",
    "import ganocracy\n",
    "from ganocracy.data import datasets as dset\n",
    "from ganocracy.data import transforms\n",
    "from ganocracy import metrics, models\n",
    "from ganocracy.models import utils as mutils\n",
    "from ganocracy.utils import visualizer as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import a small package called ganocracy, which aggregates a host of useful GAN specific functions and utilities in one place for everything needed to get started with GANs. Almost all code shown in this notebook can be found inside the ganocracy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK-SPECIFIC IMPORTS\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, Video\n",
    "try:\n",
    "    import moviepy.editor as mpy\n",
    "except ImportError:\n",
    "    print('WARNING: Could not import moviepy. Some cells may not work.')\n",
    "    print('You can install it with `pip install moviepy`')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional libraries to allow us to display the output that we create through our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seem for reproducibility.\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Use this command to make a subset of\n",
    "# GPUS visible to the jupyter notebook.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we configure how the notebook will run and set it up with our default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "\n",
    "# ---------------------\n",
    "# DATASET/Loader CONFIG\n",
    "# ---------------------\n",
    "dataset_name = 'CelebA' # Name of dataset.\n",
    "dataroot = 'data'       # Root directory for the dataset.\n",
    "download = True         # If data is not found, download and cache it.\n",
    "split = 'train'         # Dataset split (train or val), if applicable.\n",
    "num_workers = 1         # Number of workers for dataloader\n",
    "batch_size = 96         # Batch size per forward/backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The number of workers allow us to break our computations into parallel run operations\n",
    "Batch size is the number of samples we propogate through the network at one time. A higher batch size decreases training time, however, it takes more memory and can degrade the quality of the output.\n",
    "\n",
    "[![Batch Size Graph](assets/optimum-batch-size.png)](https://www.boost.co.nz/blog/2018/11/reduce-batch-size-agile-software-development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 128  # Image size (H, W) in pixels. Choices: [32, 64, 128, ...] Resized if needed.\n",
    "\n",
    "# ------------------\n",
    "# Model Architecture\n",
    "# ------------------\n",
    "dim_z = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "G_ch = 64   # Channel multiplier - scales number of features per conv in G and D\n",
    "D_ch = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension of z determines the dimension of z is the random noise that will be act as a starting point for our network. Changing the channel multiplier also changes the generator and descriminator's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# TRAINING CONFIG\n",
    "# ---------------\n",
    "num_epochs = 50 # Number of training epochs (1 epoch = 1 presentation of full dataset).\n",
    "\n",
    "num_D_steps = 1         # Number of updates to Discriminator per 1 step of Generator.\n",
    "num_D_accumulations = 1 # Number of gradient accumulations per step of G and D.\n",
    "num_G_accumulations = 1 # Technique to spoof larger \"mega-batch\" sizes.\n",
    "D_batch_size = batch_size * num_D_steps * num_D_accumulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs represents how many times we run through all of our data. \n",
    "By changing the number of updates to our descriminator per step of the generator we can fine tune the training of the descriminator so it doesn't overpower our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_every = 500   # How frequently to evaluate our model. Use None to skip testing.\n",
    "sample_every = 100 # How frequently we save fixed noise samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment LIVE_DEMO_CelebA_128_bs96_dim_z100_G_ch64_D_ch64_G_lr0.0002_D_lr0.0002_G_betas0.5_0.999_D_betas0.5_0.999\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# OPTIMIZER CONFIG\n",
    "# ----------------\n",
    "G_lr = 2e-4      # Learning rate for optimizers\n",
    "D_lr = 2e-4      \n",
    "\n",
    "# Betas hyperparams for Adam optimizers\n",
    "G_betas = (0.5, 0.999)\n",
    "D_betas = (0.5, 0.999)\n",
    "\n",
    "# ---------------\n",
    "# HARDWARE CONFIG\n",
    "# ---------------\n",
    "ngpu = 6 # Number of GPUs available. Use 0 for CPU mode\n",
    "\n",
    "# Determine whether or not GPUs are available.\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "\n",
    "# -----------\n",
    "# SAVE CONFIG\n",
    "# -----------\n",
    "name = 'LIVE_DEMO'         # Experiment specific prefix.\n",
    "model_dir = 'checkpoints'  # Directory to store model checkpoints.\n",
    "samples_dir = 'samples'    # Directory to store samples.\n",
    "\n",
    "# Generate model name based on config.\n",
    "model_name = '_'.join(map(str, [\n",
    "    name,\n",
    "    dataset_name, \n",
    "    resolution,\n",
    "    'bs{}'.format(batch_size),\n",
    "    'dim_z{}'.format(dim_z),\n",
    "    'G_ch{}'.format(G_ch),\n",
    "    'D_ch{}'.format(D_ch),\n",
    "    'G_lr{}'.format(G_lr),\n",
    "    'D_lr{}'.format(D_lr),\n",
    "    'G_betas{}'.format('_'.join(map(str, G_betas))),\n",
    "    'D_betas{}'.format('_'.join(map(str, D_betas))),\n",
    "]))\n",
    "\n",
    "# Prepare dirs to store samples and checkpoints.\n",
    "save_name = os.path.join(model_dir, model_name)\n",
    "os.makedirs(os.path.join(model_dir, model_name), exist_ok=True)\n",
    "os.makedirs(os.path.join(samples_dir, model_name), exist_ok=True)\n",
    "print(f'Starting experiment {model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this tutorial, we will use the CelebFaces Attributes Dataset (CelebA), a large-scale face attributes dataset with more than 200K celebrity images. Each image is annotated with 40 attributes (e.g. smiling, eyeglasses, etc.). \n",
    "\n",
    "We can download this dataset (along with a host of other commonly used datasets) with PyTorch's [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html) package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CelebA\n",
      "    Number of datapoints: 162770\n",
      "    Root location: data\n",
      "    Target type: ['attr']\n",
      "    Split: train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=128, interpolation=PIL.Image.BILINEAR)\n",
      "               CenterCrop(size=(128, 128))\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "Target transform: <function <lambda> at 0x00000202D2108400>\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x00000202D2108400>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d4232c49f123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                          num_workers=num_workers)\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Let's take a look at some samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mvutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\MIT\\Bridge\\K12-Notebooks-GAN\\Gan-training\\ganocracy\\utils\\visualizer.py\u001b[0m in \u001b[0;36mvisualize_data\u001b[1;34m(data, num_samples, figsize, title)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         raise ValueError(f'Unrecognized data source type: {type(data)}'\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    558\u001b[0m                 \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bdalb\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x00000202D2108400>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "# Create the dataset with transforms.\n",
    "dataset = torchvision.datasets.CelebA('data',\n",
    "                                      download=download,\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.Resize(resolution),\n",
    "                                          transforms.CenterCrop(resolution),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5, 0.5, 0.5),  # Normalize between -1 and 1.\n",
    "                                                               (0.5, 0.5, 0.5))]),\n",
    "                                       target_transform=lambda x: 0)\n",
    "# Create the dataloader.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         shuffle=True,\n",
    "                                         batch_size=D_batch_size,\n",
    "                                         num_workers=num_workers)\n",
    "# Let's take a look at some samples\n",
    "vutils.visualize_data(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own data\n",
    "\n",
    "If you would like to train a GAN on a your own custom dataset, subclassing [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is a sensible approach as it allows you to make use of PyTorch's dataloading utilities, including the multi-threaded dataLoader and transforms from above.\n",
    "\n",
    "If your training data consist of image files, the [`torchvision.datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class facilitates convenient dataloading. Simply arrange your files in the following way:\n",
    "\n",
    "    root/dogball/xxx.png\n",
    "    root/dogball/xxy.png\n",
    "    root/dogball/xxz.png\n",
    "\n",
    "    root/cat/123.png\n",
    "    root/cat/nsdf3.png\n",
    "    root/cat/asd932_.png\n",
    "    \n",
    "where each subdirectory of `root` is considered an image category containing examples of that category. Then, you can create your dataset with:\n",
    "\n",
    "```python\n",
    "dataroot = '/path/to/data/root'\n",
    "dataset = torchvision.datasets.ImageFolder(dataroot transforms=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring a sample's quality during training\n",
    "This is an optional step and not always possible to do, but if you'd like to see how the quality of samples in training are measured.\n",
    "Typically, when training any sort of neural network, it is standard practice to monitor the value of the objective function (loss) throughout the course of the experiment; adversarial losses measure the competition between the generator and discriminator. However, the adversarial losses do not necessarily reflect the image quality of generated samples.\n",
    "Objectively evaluating the “realness” and “diversity” of generated images is not an easy task. It’s on ongoing research project to determine what we consider quality photos. However, two popular metrics are inception Score and Frechet Distance: \n",
    "- **[Inception Score](https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a)** (IS): Score based on how confidently an ImageNet-pretrained InceptionV3 network can classify generated samples and the diversity of its predictions over large collection of samples. If a model produces samples that InceptionV3 can confidently classify, this contributes to a higher IS. Essentially, more images with recognizable concepts in them corrospond to images that are of greater quality. A high diversity of classifications also contributes to a higher IS.\n",
    "[![Inception Score](assets/inception-score.png)](https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a)\n",
    "- **[Fréchet Distance](https://www.mathworks.com/matlabcentral/fileexchange/31922-discrete-frechet-distance)** (FID): Here, a pretrained Inception Network is used to generate feature representations for both the real images from the dataset of interest and generated samples from the model. These feature distributions are modeled by multivariate Gaussian distributions. The shorter the Fréchet Distance between these two distributions, the more closely the fake images resemble the real ones.\n",
    "[![Fréchet Distance](assets/frechet-distance.jpg)](https://www.mathworks.com/matlabcentral/fileexchange/31922-discrete-frechet-distance)\n",
    "Summary\n",
    "\n",
    "- Higher IS values mean better image quality and diversity (usually).\n",
    "- Lower FID values mean better image quality and diversity.\n",
    "\n",
    "#### Some Caveats\n",
    "The InceptionV3 model is trained to classify ImageNet categories. The ImageNet database is a source of pictures of objects in specific orientations in order to be used for classifier training.\n",
    "Since the InceptionV3 model is trained to classify ImageNet categories, Inception Score can be a very poor measure of quality on datasets other than ImageNet. Unless your dataset if very ImageNet-like, FID will likely be a better estimate of sample quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN and test_every is not None:\n",
    "    # If we are going to evaluate IS and FID, we need to precompute Inception moments.\n",
    "    inception_moments_file = metrics.calculate_inception_moments(dataloader, dataroot,\n",
    "                                                                 '-'.join([dataset_name, str(resolution)]),\n",
    "                                                                 device=device)\n",
    "    compute_inception_metrics = metrics.prepare_inception_metrics(inception_moments_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Review of Nets\n",
    "### Overfitting\n",
    "If we put too much emphasis on a specific training sets data points it is very likely that we will overfit our data. Overfitting means that we are putting too much emphasis on specific outlier points that don’t fit our overall trend. Shown below the green line is overfitting the data when we would prefer the black line to be our determining line even though the green line technically splits our training set perfectly.\n",
    "[![Overfitting](assets/overfitting.png)](https://en.wikipedia.org/wiki/Overfitting)\n",
    "For example, if we add a new yellow point: Our intuition would tell us it is more likely to be blue but it is instead classified as red by the green line because of the overfitting of our network.\n",
    "### Regularization\n",
    "Regularization is the process of preventing overfitting in a neural network. Typically the function of regularization has some perception of complexity and factors into the backpropagation to determine the best fit. A good regularization would come up with a differentiator more akin to the black line as opposed to the green line in the above example of overfitting.\n",
    "### Multilayer perceptron\n",
    "In machine learning a multilayer perceptron is a neuron connected to all neurons of the next layer in the network.\n",
    "\n",
    "[![Multilayer Perceptron](assets/Multilayer-Perceptron.png)](https://www.researchgate.net/figure/A-hypothetical-example-of-Multilayer-Perceptron-Network_fig4_303875065)\n",
    "The problem with a network with this structure is that it is very easy to overfit since there are so many possibly unnecessary connections.\n",
    "### Convolutional Neural Networks\n",
    "Convolutional neural networks are based off the idea of a multilayer perceptron but include a form of structural regularization in order to prevent overfitting. The theory behind them is to build more complicated network structures from less complicated structures. They are particularly good at image classification because they emulate how the human brain sends signals from the animal visual cortex. They also need relatively little pre-processing to classify images which is a major advantage compared to their image classifying counterparts.\n",
    "\n",
    "[![DCGAN Generator Architecure](assets/Convolutional.jpeg)](https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional GANS \n",
    "\n",
    "With all that background out of the way we will now introduce convolutional GANS. DCGANs work by modeling G (generator) and D (discrimator) functions with convolutional neural networks (CNNs). DCGANs stands as one of the most popular and successful baseline GAN architectures because CNN's adeptness at working with images. The CNNs are also more robust and in the GAN training process.\n",
    "\n",
    "**Challenge:** Design a neural network architecture for efficient and stable image generation. \n",
    "\n",
    "**Solution:** A fully convolutional network that does away with max pooling (DCGANs successfully eliminate connecting all of the neurons of the layers of the network as a normal CNN does). Convolutions had already proven successful for discriminative computer vision tasks since they are well suited for handling the spatial structure of images, and were introduced to GANs in this paper.\n",
    "\n",
    "Below is a figure depicting the design of the Generator:\n",
    "![DCGAN Generator Architecure](assets/dcgan_generator.png)\n",
    "\n",
    "Almost all GANs used for image synthesis build on ideas introduced by DCGAN, although some modern architectures are starting to deviate from these early design decisions. None-the-less, due to its simplicity and success, DCGAN remains a good starting point for a new project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "In this implementation, we will build the DCGAN generator out of GBlocks which are a block of weighted units. Using blocks increases the spatial dimensions while decreasing the feature volume depth. \n",
    "\n",
    "Essentially, we can increase the desired output resolution by simply adding more blocks to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f5c2fd64f6de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mGBlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGblock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mGBlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MIT\\Bridge\\K12-Notebooks-GAN\\Gan-training\\GBlock.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mGBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         self.conv = nn.ConvTranspose2d(in_channels, out_channels,\n\u001b[0;32m      5\u001b[0m                                        \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from GBlock.py import Gblock\n",
    "from GBlock.py import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DBlocks`, found in the discriminator, are near inverses to `GBlocks`, trading spatial dimension for feature depth, with the exception of using LearkyReLUs instead of ReLUs for their nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-79a0b85434ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mDBlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDBlock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mDBlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MIT\\Bridge\\K12-Notebooks-GAN\\Gan-training\\DBlock.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         self.conv = nn.Conv2d(in_channels, out_channels, \n\u001b[0;32m      5\u001b[0m                               \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from DBlock.py import DBlock\n",
    "from DBlock.py import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a new GAN, it can really useful to see exactly how the shape of the data changes as it is transformed from noise to RGB image. We provide a small utility for capturing the intermediate sizes and use it below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DCGAN Generator instance.\n",
    "G = Generator(dim_z=dim_z, resolution=resolution).to(device)\n",
    "print(G)  # View model architecture.\n",
    "\n",
    "# Create some input tensors.\n",
    "z = torch.randn(4, G.dim_z, device=device)\n",
    "\n",
    "# Generate samples while capturing the sizes of intermediate outputs.\n",
    "Gz, names, sizes = mutils.hook_sizes(G, inputs=(z,), verbose=True)\n",
    "vutils.visualize_samples(Gz, title='Samples from untrained Generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "D = models.dcgan.Discriminator(resolution=resolution).to(device)\n",
    "print(D)  # View discriminator architecture.\n",
    "\n",
    "# Let's pass the output of the generator to the dicriminator.\n",
    "D_Gz, names, sizes = mutils.hook_sizes(D, inputs=(Gz,), verbose=True)\n",
    "print('D(G(z)) should be probabilities: {}'.format(D_Gz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "Recall, the adverarial training strategy is to define a game between two competing networks:\n",
    "- **generator:** a function $G$ that spawns ‘fake’ images by mapping a sorce of noise to the input space.\n",
    "- **discriminator** a function $D$ that must distinguish between a generated sample or a true data sample.\n",
    "In a zero-sum, non-cooperative game, the generator is trained to fool the discriminator\n",
    "\n",
    "In practice, it's better to start with the non-saturating GAN variant, the generator. The intuition for the non-saturating variant is that, in practice, it's easier to optimize the discriminator than the generator, especially early on in training. \n",
    "\n",
    "If the generator is not doing a good job yet, then the discriminator will continually tell the generator that it is not creating real images and the generator has no positive feedback in order to improve itself. This is a case of vanishing gradients in 𝐺\n",
    "\n",
    "These are the functions that each network uses to judge its performance.\n",
    "\n",
    "Discriminator maximizes: $\\mathbb{E}_{x\\sim p_{data}(x)}\\big[\\log D(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[\\log(1-D(G(z))\\big]$\n",
    "\n",
    "Generator maximizes: $\\mathbb{E}_{x\\sim p_{z}(z)}\\big[\\log(D(G(z)))\\big]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function.\n",
    "criterion = nn.BCELoss()\n",
    "# Note: criterion(x, 1) = -log(x)\n",
    "#       criterion(x, 0) = -log(1 - x)\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "# the progression of the generator.\n",
    "fixed_noise = torch.randn(batch_size, dim_z, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "G_optim = optim.Adam(G.parameters(), lr=G_lr, betas=G_betas)\n",
    "D_optim = optim.Adam(D.parameters(), lr=D_lr, betas=D_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training, let's prepare to monitor important quantities. In practice, you would also want to log these to disk, tensorboard, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a running list of various quantities:\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "IS_scores = []\n",
    "FIDs = []\n",
    "iters = 0\n",
    "best_IS = 0\n",
    "best_FID = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will start our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "if TRAIN:\n",
    "    print(\"Starting Training Loop...\")\n",
    "    \n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            counter = 0 # Keep track of \"mini-batches\"\n",
    "            data = [d.to(device) for d in data]\n",
    "            x, y = [torch.split(d, batch_size) for d in data]\n",
    "            \n",
    "            real_labels = torch.full((batch_size,), real_label, device=device)\n",
    "            fake_labels = torch.full((batch_size,), fake_label, device=device) \n",
    "            \n",
    "            #################################################################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            #################################################################\n",
    "\n",
    "            D.zero_grad()            \n",
    "            for step_index in range(num_D_steps):\n",
    "                D.zero_grad()\n",
    "                for accumulation_index in range(num_D_accumulations):\n",
    "                    \n",
    "                    ## ** Train with all-real batch ** ##\n",
    "\n",
    "                    # Forward real batch through D\n",
    "                    output = D(x[counter]).view(-1)\n",
    "                    \n",
    "                    # Calculate loss on all-real batch\n",
    "                    D_loss_real = criterion(output, real_labels[:output.size(0)]) / float(num_D_accumulations)\n",
    "                    D_loss_real.backward()\n",
    "                    D_x = output.mean().item()\n",
    "\n",
    "                    ## ** Train with all-fake batch ** ##\n",
    "                \n",
    "                    # Generate batch of latent vectors and targets\n",
    "                    noise = torch.randn(batch_size, dim_z, device=device)\n",
    "                    \n",
    "                    # Generate fake image batch with G\n",
    "                    fake_image = G(noise)\n",
    "\n",
    "                    # Classify all fake batch with D\n",
    "                    output = D(fake_image.detach()).view(-1)\n",
    "\n",
    "                    # Calculate D's loss on the all-fake batch\n",
    "                    D_loss_fake = criterion(output, fake_labels[:output.size(0)]) / float(num_D_accumulations)\n",
    "                    D_loss_fake.backward()\n",
    "                    D_G_z1 = output.mean().item()\n",
    "                    \n",
    "                    # Add the gradients from the all-real and all-fake batches\n",
    "                    D_loss = (D_loss_real + D_loss_fake)\n",
    "                    counter += 1\n",
    "                    \n",
    "                # Update D\n",
    "                D_optim.step()\n",
    "\n",
    "            #############################################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            #############################################\n",
    "            \n",
    "            G.zero_grad()\n",
    "            for accumulation_index in range(num_G_accumulations):\n",
    "                \n",
    "                # Generate batch of latent vectors.\n",
    "                noise = torch.randn(batch_size, dim_z, device=device)\n",
    "                fake_image = G(noise)\n",
    "                output = D(fake_image).view(-1)\n",
    "                \n",
    "                # Calculate G's loss based on this output\n",
    "                G_loss = criterion(output, real_labels) / float(num_G_accumulations)\n",
    "                    \n",
    "                # Calculate gradients for G\n",
    "                G_loss.backward()\n",
    "                D_G_z2 = output.mean().item()\n",
    "                \n",
    "            # Update G\n",
    "            G_optim.step()\n",
    "            \n",
    "            # Output training stats\n",
    "            if i % 10 == 0:\n",
    "                print('[{}/{}][{}/{}]\\tLoss_D: {:.4f}\\tLoss_G: {:.4f}\\tD(x): {:.4f}\\tD(G(z)): {:.4f} / {:.4f}'.\n",
    "                     format(epoch, num_epochs, i, len(dataloader),\n",
    "                            D_loss.item(), G_loss.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(G_loss.item())\n",
    "            D_losses.append(D_loss.item())\n",
    "\n",
    "            # Check how the generator is doing by saving G's samples on fixed_noise\n",
    "            if (iters % sample_every == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "                with torch.no_grad():\n",
    "                    fake_image = G(fixed_noise).detach().cpu()\n",
    "                    \n",
    "                # Save to disk and keep copy in list\n",
    "                fname = os.path.join(samples_dir, model_name, f'{iters:06d}.jpg')\n",
    "                torchvision.utils.save_image(fake_image, fname, padding=2, normalize=True )\n",
    "                img_list.append(torchvision.utils.make_grid(fake_image, padding=2, normalize=True))\n",
    "            \n",
    "            if test_every is not None:\n",
    "                if ((iters % test_every == 0)\n",
    "                    or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1))):\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        IS, IS_std,  FID = compute_inception_metrics(G, batch_size, dim_z)\n",
    "\n",
    "                    print('Itr {}: PYTORCH Inception Score is {:3.3f} +/- {:3.3f} '\n",
    "                          'PYTORCH FID is {:5.4f}'.format(iters, IS, IS_std, FID))\n",
    "                    IS_scores.append(IS)\n",
    "                    FIDs.append(FID)\n",
    "                    \n",
    "                    # Remember best IS and FID and save checkpoint.\n",
    "                    is_best_IS = IS > best_IS\n",
    "                    is_best_FID = FID < best_FID\n",
    "                    best_IS = max(IS, best_IS)\n",
    "                    best_FID = min(FID, best_FID)\n",
    "                    \n",
    "                    # Keep a running checkpoint\n",
    "                    mutils.save_checkpoint({\n",
    "                        'G': G.state_dict(),\n",
    "                        'D': D.state_dict(),\n",
    "                        'iters': iters,\n",
    "                        'epoch': epoch,\n",
    "                        'IS_scores': IS_scores,\n",
    "                        'FIDs': FIDs,\n",
    "                        'best_IS': best_IS,\n",
    "                        'best_FID': best_FID,\n",
    "                    }, is_best_IS, is_best_FID, \n",
    "                        filename=save_name + '.pth.tar')\n",
    "                    \n",
    "            iters += 1\n",
    "        # Finally, save a checkpoint every epoch.\n",
    "        torch.save({\n",
    "            'G': G.state_dict(),\n",
    "            'D': D.state_dict(),\n",
    "            'G_losses': G_losses,\n",
    "            'D_losses': D_losses,\n",
    "            'iters': iters,\n",
    "            'epoch': epoch,\n",
    "            'IS_scores': IS_scores,\n",
    "            'FIDs': FIDs,\n",
    "            'best_IS': best_IS,\n",
    "            'best_FID': best_FID,\n",
    "            }, f'{save_name}_epoch{epoch}.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we're off!\n",
    "\n",
    "... And now we wait. At lower resolutions or fewer classes, it is possible to obtain farily respectable results in a short time frames. However, acheiving  the eye-catching results commonly advertised in paper and the media still takes quite a while, on the order of weeks potentially. \n",
    "\n",
    "**Note on hardware utilization**: If you are using GPUs make sure that you are using them to the fullest. The command `watch -n 1 nvidia-smi` will show utilization and memory consumption continuously (refresehed every second). Could you use a larger batch size? If you see your GPU usage cycle between 100% and 0% it could be evidence of a dataloading bottleneck.\n",
    "\n",
    "### \"Babysitting\" the learning process\n",
    "\n",
    "Given that training these models can be an investment in time and resources, it's wise to continuously monitor training in order to catch and address anamolies if/when they occur. Here are some things to look out for:\n",
    "\n",
    "#### What should the losses look like?\n",
    "\n",
    "GAN losses come in all shapes and sizes and depend on numerous factors including architecture, dataset, and loss function. The adversarial learning process is highly dynamic and high frequency oscillations are quite common:\n",
    "\n",
    "![Loss Montage](assets/loss_montage.jpg)\n",
    "\n",
    "**Recommendation:** Make sure losses fall within a reasonable range and *catch failures early!*. If either loss (D or G) skyrockets to huge values, plunge to 0 or get stuck on a single value, there is likely an issue somewhere.If you are training a common architecture, consult the literature/other implementations to ground your expectations. One of the hardest things about re-implementing a paper can be checking if the logs line up early in training, especially if training takes multiple weeks.\n",
    "\n",
    "**Is my model learning?**\n",
    "- Monitor IS and FID metrics and other image quality metrics (if applicable) - are they following the expected trajectories? \n",
    "- How do the samples look? Are they improving over time? Do you see evidence of mode collapse?\n",
    "\n",
    "*Mode Collapse*: When the generator produces an extremely limited set output patterns (\"modes\") despite maintaining diversity of input noise. Here are samples generated by a model undergoing mode collapse:\n",
    "\n",
    "![Mode Collapse](assets/collapse.jpg)\n",
    "\n",
    "**How do I know when to stop?**\n",
    "- Most importantly, do the samples meet your expectations?\n",
    "- Sharp increase in metrics followed by collapse?\n",
    "- No longer improving.\n",
    "- Explore your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now that we have finished training, let's find out how we did. We will analyze our model in several ways:\n",
    "1. Examine how D and G’s losses changed during training.\n",
    "2. Visualize G’s output on the fixed_noise batch for every epoch and create a video.\n",
    "3. Explore what the Generator has learned in its latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you did not train, but want to continue with \n",
    "# a pretrained model + logs.\n",
    "if not TRAIN:\n",
    "    url = 'http://ganocracy.csail.mit.edu/models/DCGAN_CelebA_128_dim_z100_G_ch64_D_ch64_G_lr0.0002_D_lr0.0002_G_betas0.5_0.999_D_betas0.5_0.999-93ba4eb0.pth'\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(url, map_location='cpu')\n",
    "    \n",
    "    # PyTorch Tip: DataParallel prepends 'module.' to state dict names.\n",
    "    # If you try to load a model trained with DataParallel into the original,\n",
    "    # you may have to strip away the prefix like so:\n",
    "    G.load_state_dict({k.replace('module.', ''): v for k, v in checkpoint['G'].items()})\n",
    "    D.load_state_dict({k.replace('module.', ''): v for k, v in checkpoint['D'].items()})\n",
    "    iters = checkpoint['iters']\n",
    "    G_losses, D_losses = checkpoint['G_losses'], checkpoint['D_losses']\n",
    "    IS_scores, FIDs = checkpoint['IS_scores'], checkpoint['FIDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_logs(G_loss, D_loss, figsize=(15, 5), smoothing=0.001):\n",
    "    \"\"\"Utility for plotting losses with smoothing.\"\"\"\n",
    "    G_loss = vutils.smooth_data(G_loss, amount=smoothing)\n",
    "    D_loss = vutils.smooth_data(D_loss, amount=smoothing)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(D_loss, label='D_loss')\n",
    "    plt.plot(G_loss, label='G_loss')\n",
    "    plt.legend(loc='lower right', fontsize='medium')\n",
    "    plt.xlabel('Iteration', fontsize='x-large')\n",
    "    plt.ylabel('Losses', fontsize='x-large')\n",
    "    plt.title('Training History', fontsize='xx-large')\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_logs(G_losses, D_losses, figsize=(15, 5), smoothing=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(num_iters, IS_scores, FIDs):\n",
    "    fig, axs = plt.subplots(2, sharex=True, figsize=(15, 5))\n",
    "    xscale = test_every if test_every is not None else (num_iters // len(IS_scores))\n",
    "    itrs = np.arange(0, len(IS_scores)) * xscale\n",
    "    axs[0].plot(itrs, IS_scores)\n",
    "    axs[1].plot(itrs, FIDs)\n",
    "    \n",
    "    for label, ax in zip(['Inception Score', 'FID'], axs.flat):\n",
    "        ax.set(ylabel=label)\n",
    "    \n",
    "    plt.xlabel('Iteration', fontsize='x-large')\n",
    "    axs[0].set_title('Training History', fontsize='xx-large')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_metrics(iters,IS_scores, FIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of G’s progression**\n",
    "\n",
    "Remember how we saved the generator’s output on the fixed_noise batch\n",
    "after every `sample_every` iterations of training. Now, we can visualize the training\n",
    "progression of G with a video. Press the play button to start the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_video(samples_dir, resolution, num_rows=8, fps=10):\n",
    "    files = sorted([os.path.join(samples_dir, f) for f in os.listdir(samples_dir) if f.endswith('.jpg')])\n",
    "    frames = [np.array(Image.open(f).convert('RGB'))[:num_rows * resolution] for f in files]\n",
    "    clip = mpy.ImageSequenceClip(frames, fps=10)\n",
    "    video_outfile = os.path.join(samples_dir, 'progress.mp4')\n",
    "    clip.write_videofile(video_outfile)\n",
    "    return video_outfile\n",
    "\n",
    "if not TRAIN:\n",
    "    video_outfile = 'assets/training_progression.mp4'\n",
    "else:\n",
    "    video_outfile = make_training_video(os.path.join(samples_dir, model_name), resolution)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the video\n",
    "Video(video_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space exploration\n",
    "\n",
    "The original DCGAN paper showed that the latent space learned by the generator maintains smooth transitions: as you walk through z space, the resulting output transistions naturally. Let's see if this holds for our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intra-class (z only) Latent space interpolation\n",
    "num_samples = 8\n",
    "num_midpoints = 8\n",
    "minibatch_size = 8\n",
    "\n",
    "# First, choose two random coordinates in z space.\n",
    "z0 = torch.randn(num_samples, dim_z).to(device)\n",
    "z1 = torch.randn(num_samples, dim_z).to(device)\n",
    "\n",
    "# Interpolate between z0 and z1.\n",
    "zs = vutils.interp(z0, z1, num_midpoints, device=device)\n",
    "zs = zs.view(-1, zs.size(-1))\n",
    "\n",
    "# Generate samples.\n",
    "with torch.no_grad():\n",
    "    samples = G(zs)\n",
    "\n",
    "# Show\n",
    "vutils.visualize_samples(samples, nrow=num_midpoints + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool implementations of gans below\n",
    "### Open source implementations\n",
    "For some awesome open-source PyTorch implementations of recent GANs, check out:\n",
    "- [BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch): Train BigGANs from *Large Scale GAN Training for High Fidelity Natural Image Synthesis* on 4-8 GPUs\n",
    "- [pytorch_GAN_zoo](https://github.com/facebookresearch/pytorch_GAN_zoo): Implementations of DCGAN and Progressive Growing of GAN by Facebook Research\n",
    "- [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix): Image-to-image translation in PyTorch (e.g., horse2zebra, edges2cats, and more)\n",
    "- [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN): Library of reference implementations (good for educational purposes)\n",
    "\n",
    "Some more information about GANs:\n",
    "- [Machine Learning](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)\n",
    "- [Types of Machine Learning](https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861)\n",
    "- [Multi Layer Perceptrons](http://deeplearning.net/tutorial/mlp.html)\n",
    "- [Reinforcement Learning](https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/)\n",
    "- [Convolutional Neural Networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "And for the Tensorflow and Keras folks:\n",
    "- [StyleGAN](https://github.com/NVlabs/stylegan): Official TensorFlow Implementation\n",
    "- [Progressive Growing of GANs](https://github.com/tkarras/progressive_growing_of_gans): Official TensorFlow Implementation.\n",
    "- [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN): Library of reference implementations (good for educational purposes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
