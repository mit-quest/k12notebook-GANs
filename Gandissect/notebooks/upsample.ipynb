{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An upsampler is just a function that we use to upsample an internal (low-resolution) convolutional signal to an external (high-resolution) coordinate system.  For modern convnets that use (n-1)/2 padding, this is just the same as corner-to-corner scaling.  However, if you use a convolutional kernel with different padding, the upsampler can align the grid to make sure that the center of the receptive field is mapped to the correct location on the image.\n",
    "\n",
    "For example, in the below, we create an AlexNet (that uses zero padding), then we create an upsampler that correctly upsamples the conv5 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.hub\n",
    "\n",
    "model = torchvision.models.alexnet(num_classes=365)\n",
    "url = 'http://gandissect.csail.mit.edu/models/alexnet_places365-6d3c0e75.pth'\n",
    "sd = torch.hub.load_state_dict_from_url(url)['state_dict']\n",
    "sd = {k.replace('.module', ''): v for k, v in sd.items()}\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "from netdissect import nethook\n",
    "model = nethook.InstrumentedModel(model)\n",
    "model = model.cuda()\n",
    "model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is an instrumented AlexNet whose structure can be seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from netdissect import parallelfolder, renormalize\n",
    "from torchvision import transforms\n",
    "from urllib.request import urlopen\n",
    "\n",
    "reload(parallelfolder)\n",
    "\n",
    "center_crop = transforms.Compose([\n",
    "        transforms.Resize((256 ,256)),\n",
    "        transforms.CenterCrop(227),\n",
    "        transforms.ToTensor(),\n",
    "        renormalize.NORMALIZER['imagenet']\n",
    "])\n",
    "\n",
    "dataset = parallelfolder.ParallelImageFolders(\n",
    "    ['dataset/places/val'], transform=[center_crop],\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
    "classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have loaded some data, that we will center-crop to 227x227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize, show\n",
    "from matplotlib import cm\n",
    "\n",
    "indices = [300]\n",
    "model.retain_layer(('features.10', 'conv5'))\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "show.blocks([[im], name] for im, name in zip(imgs, prednames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we run the model on a small batch (just one image, image number 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import upsample\n",
    "from torch.nn import MaxPool2d\n",
    "import PIL\n",
    "reload(upsample)\n",
    "\n",
    "acts = model.retained_layer('conv5').cpu()\n",
    "\n",
    "# Define the upsampler function.\n",
    "upfn = upsample.upsampler(\n",
    "    (56, 56),                     # The target output shape\n",
    "    acts.shape[2:],               # The source data shape\n",
    "    input_shape=batch.shape[2:],  # The actual input data was this shape\n",
    "    convolutions=list(model.model.features.children())[:10]  # The conv modules from input to data reveal strides\n",
    ")\n",
    "\n",
    "upacts = upfn(acts, mode='nearest')\n",
    "show.blocks(\n",
    "    [PIL.Image.fromarray((cm.hot(a/(1e-10 +a.max())) * 255).astype('uint8'))]\n",
    "    for a in upacts[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we upsample the activation to 56x56 (quarter resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "reload(imgviz)\n",
    "iv = imgviz.ImageVisualizer((56, 56), source=dataset, convolutions=list(model.model.features.children())[:10])\n",
    "show.blocks(\n",
    "    [[[iv.masked_image(batch[0], acts, (0, u))],\n",
    "      [iv.heatmap(acts, (0, u), mode='nearest')]] for u in range(acts.shape[1])]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}