{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Dissection (for ResNet152)\n",
    "\n",
    "In this notebook, we will examine internal layer representations for a classifier trained to recognize scene categories.\n",
    "\n",
    "Setup matplotlib, torch, and numpy for a high-resolution browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load resnet pretrained on places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.hub\n",
    "from netdissect import oldresnet152\n",
    "model = oldresnet152.OldResNet152()\n",
    "# url = 'http://gandissect.csail.mit.edu/models/resnet18_places365-2f475921.pth'\n",
    "# url = 'http://gandissect.csail.mit.edu/models/resnet50_places365-46529c86.pth'\n",
    "url = 'http://gandissect.csail.mit.edu/models/resnet152_places365-f928166e5c.pth'\n",
    "try:\n",
    "    sd = torch.hub.load_state_dict_from_url(url) # pytorch 1.1\n",
    "except:\n",
    "    sd = torch.hub.model_zoo.load_url(url) # pytorch 1.0\n",
    "# sd = sd['state_dict']\n",
    "# sd = {k.replace('module.', ''): v for k, v in sd.items()}\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "from netdissect import nethook\n",
    "model = nethook.InstrumentedModel(model)\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "from urllib.request import urlopen\n",
    "\n",
    "synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
    "classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import segmenter\n",
    "segmodel = segmenter.UnifiedParsingSegmenter(segsizes=[256])\n",
    "seglabels = [l for l, c in segmodel.get_label_and_category_names()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from netdissect import parallelfolder, renormalize\n",
    "from torchvision import transforms\n",
    "\n",
    "reload(parallelfolder)\n",
    "\n",
    "center_crop = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        renormalize.NORMALIZER['imagenet']\n",
    "])\n",
    "\n",
    "dataset = parallelfolder.ParallelImageFolders(\n",
    "    ['dataset/places/val'], transform=[center_crop],\n",
    "    classification=True,\n",
    "    shuffle=True)\n",
    "\n",
    "train_dataset = parallelfolder.ParallelImageFolders(\n",
    "    ['dataset/places/train'], transform=[center_crop],\n",
    "    classification=True,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test classifier on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize\n",
    "\n",
    "indices = [200, 755, 709, 423] #range(200,224)\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "truenames = [classlabels[dataset[i][1]] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import show\n",
    "\n",
    "show([[img, pred, tn] for img, pred, tn in zip(imgs, prednames, truenames)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create adapter to segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize\n",
    "reload(renormalize)\n",
    "renorm = renormalize.renormalizer(dataset, mode='zc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segment single image, and visualize the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz, upsample, segviz\n",
    "reload(segviz)\n",
    "\n",
    "iv = imgviz.ImageVisualizer(120, source=dataset)\n",
    "            \n",
    "seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
    "\n",
    "show([(iv.image(batch[i]), iv.segmentation(seg[i,0]),\n",
    "            iv.segment_key(seg[i,0], segmodel))\n",
    "            for i in range(len(seg))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize activations for single layer of single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.stop_retaining_layers([('features.8', 'conv5')])\n",
    "model.retain_layer(('7'))\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "\n",
    "acts = model.retained_layer('7').cpu()\n",
    "ivsmall = imgviz.ImageVisualizer((56, 56), source=dataset)\n",
    "show.blocks(\n",
    "    [[[ivsmall.masked_image(batch[0], acts, (0, u))],\n",
    "      [ivsmall.heatmap(acts, (0, u), mode='nearest')]] for u in range(max(acts.shape[1], 50))]\n",
    ")\n",
    "unit_count = acts.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect quantile statistics\n",
    "\n",
    "First, unconditional quantiles over the activations.  We will upsample them to 56x56 to match with segmentations later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "upfn = upsample.upsampler(\n",
    "    (56, 56),                     # The target output shape\n",
    "    (7, 7),\n",
    "    source=dataset,\n",
    ")\n",
    "\n",
    "def compute_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer('7')\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "\n",
    "\n",
    "rq = tally.tally_quantile(compute_samples, dataset, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's collect bincounts of the segmentations, also unconditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally, upsample\n",
    "\n",
    "def compute_segments(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    return seg\n",
    "\n",
    "from netdissect import tally\n",
    "\n",
    "segbc = tally.tally_bincount(compute_segments,\n",
    "        dataset, sample_size=1000, multi_label_axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main statistic: condq is the conditional quantile statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    from netdissect import tally\n",
    "    reload(tally)\n",
    "\n",
    "    def compute_conditional_samples(batch, *args):\n",
    "        image_batch = batch.cuda()\n",
    "        _ = model(image_batch)\n",
    "        acts = model.retained_layer('7')\n",
    "        seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "        hacts = torch.nn.functional.interpolate(acts, size=seg.shape[2:],\n",
    "                                                mode='bilinear', align_corners=False)\n",
    "        hacts = upfn(acts)\n",
    "        return tally.conditional_samples(hacts, seg)\n",
    "\n",
    "\n",
    "    condq = tally.tally_conditional_quantile(compute_conditional_samples,\n",
    "            dataset, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condvar is a conditional mean (and variance) statistic, collected directly so it's more accurate than you can obtain from the quantile sketch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "condvar = tally.tally_conditional_mean(compute_conditional_samples,\n",
    "        dataset, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein distance experiment.\n",
    "\n",
    "(Commented out for now) - for each segmented concept, find the unit whose behavior changes the most when the segmented concept is present, according to the wasserstein distance between the unconditional distribution and the distribution with the concept present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Compute the wasserstein distance between the unconditional and conditional distribution of the activation.\n",
    "# And then rank units by maximium wasserstein disance.\n",
    "print('Unit with largest wasserstein shift under each segmented concept')\n",
    "for conceptnum, label in enumerate(seglabels):\n",
    "    if conceptnum not in condq.keys():\n",
    "        continue\n",
    "    amt, ind = ((condq.conditional(conceptnum).readout(1000) -\n",
    "      condq.conditional(0).readout(1000)).abs().sum(1)/1000).max(0)\n",
    "    print(conceptnum, amt.item(), 'unit', ind.item(), condq.conditional(conceptnum).size(), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the wasserstein shift for one unit under a concept where the shift is large.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "segconcept = 9 # window\n",
    "unit = 1058  # largest wasserstein shift under window\n",
    "def dens(cum):\n",
    "    return cum[1:] - cum[:-1]\n",
    "baseline = condq.conditional(0).readout(1001)[unit].numpy()\n",
    "conditioned = condq.conditional(segconcept).readout(1001)[unit].numpy()\n",
    "top = max(baseline.max(), conditioned.max())\n",
    "buckets = numpy.linspace(0, top, 25)\n",
    "ax.hist(baseline, buckets, alpha=0.5)\n",
    "ax.hist(conditioned, buckets, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing max activating images.\n",
    "\n",
    "visualize high-activation regions for single image.\n",
    "\n",
    "To support this function, we need topk statistics over a sample.  This collects them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "reload(tally)\n",
    "\n",
    "\n",
    "def compute_image_max(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer('7')\n",
    "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
    "    acts = acts.max(2)[0]\n",
    "    return acts\n",
    "\n",
    "topk = tally.tally_topk(compute_image_max, dataset, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function defines a visualization of one unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "from IPython.display import display\n",
    "\n",
    "reload(imgviz)\n",
    "iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq)\n",
    "\n",
    "def unit_viz_row(unitnum, percent_level=None):\n",
    "    out = []\n",
    "    for imgnum in topk.result()[1][unitnum][:8]:\n",
    "        img = dataset[imgnum][0][None,...].cuda()\n",
    "        scores = model(img.cuda())\n",
    "        pred = classlabels[scores.max(1)[1].item()].split('/')[0]\n",
    "        acts = model.retained_layer('7')\n",
    "        out.append([# [iv.image(img[0]), pred],\n",
    "                    # [iv.heatmap(acts, (0, unitnum), mode='nearest'), str(acts[0, unitnum].max().item())[:5]],\n",
    "                    [iv.masked_image(img[0], acts, (0, unitnum), percent_level=percent_level), imgnum.item()],\n",
    "                   ])\n",
    "    return out\n",
    "display(show.blocks(unit_viz_row(444)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler version of the wasserstein shift experiment:\n",
    "\n",
    "For each concept, just show the unit that has the largest difference of conditional mean vs unconditional mean, when normalized by the unconditional mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptlist = []\n",
    "for conceptnum, label in enumerate(seglabels):\n",
    "    if not condq.has_conditional(conceptnum):\n",
    "        continue\n",
    "    ratio, index = (abs(condq.conditional(conceptnum).mean() - rq.mean()) / rq.mean()).max(0)\n",
    "    stdev = ((condq.conditional(conceptnum).stdev()) / rq.mean())[index]\n",
    "    print(label, 'unit', index.item(), 'ratio', ratio.item(), 'size', condq.conditional(conceptnum).size())\n",
    "    conceptlist.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from netdissect import bargraph\n",
    "reload(bargraph)\n",
    "from IPython.display import display, SVG, HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "def graph_conceptlist(conceptlist):\n",
    "    count = defaultdict(int)\n",
    "    for c in conceptlist:\n",
    "        count[c] += 1\n",
    "    labels, counts = zip(*sorted(count.items(), key=lambda x: -x[1]))\n",
    "    return HTML('<div style=\"height:200px;width:5000px\">' + bargraph.make_svg_bargraph(labels, counts) + '</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1.\n",
    "\n",
    "Assign a label to each unit according to the highest iou at fixed threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "iouscores_at_99 = torch.zeros((max(condq.keys()) + 1, 2048))\n",
    "# Compute at fixed quantile\n",
    "actquantile = 0.01\n",
    "actlevel = condq.conditional(0).quantiles([1 - actquantile])[:,0]\n",
    "for c in sorted(condq.keys()):\n",
    "    if c == 0 or condq.conditional(c).batchcount <= 1:\n",
    "        continue\n",
    "    levelp = condq.conditional(c).normalize(actlevel)\n",
    "    cp = float(condq.conditional(c).size()) / condq.conditional(0).size()\n",
    "    iouscores_at_99[c] = cp * (1 - levelp) / (actquantile + cp * levelp)\n",
    "conceptlist_at_99, unitlist_at_99 = [], []\n",
    "for u in range(2048):\n",
    "    iou, c = iouscores_at_99[:,u].max(0)\n",
    "    c = c.item()\n",
    "    diff = condvar.conditional(c).mean()[u] - condvar.conditional(0).mean()[u]\n",
    "    unitlist_at_99.append(dict(\n",
    "        unit=u,\n",
    "        label=seglabels[c],\n",
    "        iou=iou.item(),\n",
    "        diff=diff.item(),\n",
    "        cnt=condvar.conditional(c).batchcount,\n",
    "    ))\n",
    "    conceptlist_at_99.append(seglabels[c])\n",
    "for d in sorted(unitlist_at_99, key=lambda x: -x['iou'])[:20]:\n",
    "    display(show.blocks([[d['label'],\n",
    "                          'iou %.2f' % d['iou'],\n",
    "                          'dm %.2f' % d['diff'],\n",
    "                          'cnt %d' % d['cnt'],\n",
    "                          'unit %d' % d['unit']]] + unit_viz_row(d['unit'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in unitlist_at_99:\n",
    "    if d['unit'] in [1242, 636]:\n",
    "        display(show.blocks([[d['label'],\n",
    "                          'iou %.2f' % d['iou'],\n",
    "                          'dm %.2f' % d['diff'],\n",
    "                          'cnt %d' % d['cnt'],\n",
    "                          'unit %d' % d['unit']]] + unit_viz_row(d['unit'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iouscores_at_99 > 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph_conceptlist([d['label'] for d in unitlist_at_99])) #  if d['iou'] > 0.05]))\n",
    "print(str(len([d['label'] for d in unitlist_at_99 if d['iou'] > 0.05])) + ' units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptlist_at_99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1.2:\n",
    "\n",
    "Now repeat, but adjust thresholds so that we measure each segmented concept at a quantile that matches the pixel frequency of that concept in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from netdissect import pbar\n",
    "\n",
    "iouscores_at_match = torch.zeros((max(condq.keys()) + 1, unit_count))\n",
    "# Compute at matching quantile\n",
    "concept_percent_level = torch.zeros(max(condq.keys()) + 1)\n",
    "\n",
    "print('step 1')\n",
    "for c in pbar(sorted(condvar.keys())):\n",
    "    if c == 0 or condvar.conditional(c).batchcount <= 1:\n",
    "        continue\n",
    "    cp = float(condq.conditional(c).size()) / condq.conditional(0).size()\n",
    "    concept_percent_level[c] = 1 - cp\n",
    "    actquantile = cp\n",
    "    actlevel = condq.conditional(0).quantiles([1 - actquantile])[:,0]\n",
    "    levelp = condq.conditional(c).normalize(actlevel)\n",
    "    iouscores_at_match[c] = cp * (1 - levelp) / (actquantile + cp * levelp)\n",
    "print('step 2')\n",
    "conceptlist_at_match, unitlist_at_match = [], []\n",
    "for u in pbar(range(unit_count)):\n",
    "    iou, c = iouscores_at_match[:,u].max(0)\n",
    "    c = c.item()\n",
    "    diff = condvar.conditional(c).mean()[u] - condvar.conditional(0).mean()[u]\n",
    "    unitlist_at_match.append(dict(\n",
    "        unit=u,\n",
    "        label=seglabels[c],\n",
    "        iou=iou.item(),\n",
    "        diff=diff.item(),\n",
    "        percent_level=concept_percent_level[c].item(),\n",
    "        cnt=condvar.conditional(c).batchcount,\n",
    "    ))\n",
    "    conceptlist_at_match.append(seglabels[c])\n",
    "for d in sorted(unitlist_at_match, key=lambda x: -x['iou'])[:20]:\n",
    "    display(show.blocks([[d['label'],\n",
    "                          'iou %.2f' % d['iou'],\n",
    "                          'per %.2f' % d['percent_level'],\n",
    "                          'dm %.2f' % d['diff'],\n",
    "                          'cnt %d' % d['cnt'],\n",
    "                          'unit %d' % d['unit']]] + unit_viz_row(d['unit'], percent_level=d['percent_level'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sorted(unitlist_at_match, key=lambda x: -x['iou'])[:20]:\n",
    "    display(show.blocks([[d['label'],\n",
    "                          'iou %.2f' % d['iou'],\n",
    "                          'per %.2f' % d['percent_level'],\n",
    "                          'dm %.2f' % d['diff'],\n",
    "                          'cnt %d' % d['cnt'],\n",
    "                          'unit %d' % d['unit']]] + unit_viz_row(d['unit'], percent_level=d['percent_level'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(condq.conditional(20).size()) / condq.conditional(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conceptlist(conceptlist_at_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2.\n",
    "\n",
    "Assign a label to each unit according to the shift in conditional mean with highest statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "zscores = torch.zeros((max(condvar.keys()) + 1, 512))\n",
    "for c in sorted(condvar.keys()):\n",
    "    if c == 0 or condvar.conditional(c).batchcount <= 1:\n",
    "        continue\n",
    "    zscores[c] = ((condvar.conditional(c).mean() - condvar.conditional(0).mean()) /\n",
    "        (condvar.conditional(0).variance() / condvar.conditional(0).batchcount\n",
    "         + condvar.conditional(c).variance() / condvar.conditional(c).batchcount).sqrt())\n",
    "conceptlist_by_zscore = []\n",
    "for u in range(256):\n",
    "    zt, c = zscores[:,u].max(0)\n",
    "    c = c.item()\n",
    "    diff = condvar.conditional(c).mean()[u] - condvar.conditional(0).mean()[u]\n",
    "    display(show.blocks([[seglabels[c],\n",
    "                          'dm %.2f' % diff.item(),\n",
    "                          'zs %.2f' % zt.item(),\n",
    "                          'cnt %d' % condvar.conditional(c).batchcount,\n",
    "                          'unit %d' % u]] + unit_viz_row(u)))\n",
    "    conceptlist_by_zscore.append(seglabels[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conceptlist(conceptlist_by_zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3.\n",
    "\n",
    "Assign a label according to the highest relative mutual information at a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination metric.\n",
    "\n",
    "Given only the number of pixels in a given segmentation, how accurately can we do binary classification on a particular scene class?\n",
    "\n",
    "To answer this, we can use conditional quantile information.\n",
    "\n",
    "Conditioned on each scene class, we collect the fraction of pixels in each segmentation class.\n",
    "Then at a given threshold t, the accuracy of scene classification is as follows:\n",
    "p(c | s>t) + p(~c | s < t) = p(c | s > t) + 1 - p(c| s< t)\n",
    "= p(s > t | c) * p(c)/ p(s>t) + 1 -  p(s<t|c) * p(c) / p(s<t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tally)\n",
    "\n",
    "def compute_conditional_discrimination(batch, classnum, *args):\n",
    "    assert len(batch) == 1\n",
    "    image_batch = batch.cuda()\n",
    "    # scores = model(image_batch)\n",
    "    # pred = scores.max(1)[1]\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    feat = seg.view(-1).bincount(minlength=len(seglabels)).float() / (seg.shape[3] * seg.shape[2])\n",
    "    feat[0] = 0\n",
    "    feat = feat[None,...]\n",
    "    return [(0, feat), (classnum.item() + 1, feat)]\n",
    "\n",
    "conddis = tally.tally_conditional_quantile(compute_conditional_discrimination,\n",
    "        train_dataset, sample_size=10000, num_workers=20) # TODO: switch to sample_size=50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(conddis.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conddis.conditional(0).size()\n",
    "condprob = torch.logspace(-3, 0, 10)[:-1] # p(s>t | c)\n",
    "accscores = torch.zeros((max(conddis.keys()) + 1, 256))\n",
    "\n",
    "c = 1\n",
    "print(classlabels[c])\n",
    "# if c == 0:\n",
    "#     continue\n",
    "level = 1 - conddis.conditional(c).quantiles(condprob)  # Levels at which the conditional quantile is achieved.\n",
    "segprob = conddis.conditional(0).normalize(level)\n",
    "margprob = float(conddis.conditional(c).size()) / conddis.conditional(0).size()\n",
    "acc = condprob * margprob / (segprob) + 1 - (1 - condprob) * margprob / (1 - segprob)\n",
    "acc1 = condprob * margprob / (segprob)\n",
    "acc2 = 1 - (1 - condprob) * margprob / (1 - segprob)\n",
    "#acc = condprob * 0.5 / (condprob  + segprob) # + 1 - (1 - condprob) * margprob / (1 - segprob)\n",
    "# acc = condprob * 0.5 / ((condprob+ segprob)/2) + 1 - (1 - condprob) * 0.5 / (1 - (segprob + condprob)/2)\n",
    "# acc = 1 - (1 - condprob) * 0.5 / (1 - (segprob + condprob)/2)\n",
    "print(acc.shape)\n",
    "\n",
    "s = 1\n",
    "print(seglabels[s])\n",
    "# plt.plot(condprob.numpy(), acc[s].numpy())\n",
    "# plt.plot(condprob.numpy(), acc1[s].numpy())\n",
    "# plt.plot(condprob.numpy(), 1 - acc2[s].numpy())\n",
    "level"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}