{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Dissection (for classifiers)\n",
    "\n",
    "In this notebook, we will examine internal layer representations for a classifier trained to recognize scene categories.\n",
    "\n",
    "Setup matplotlib, torch, and numpy for a high-resolution browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load alexnet pretrained on places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.hub\n",
    "\n",
    "model = torchvision.models.alexnet(num_classes=365)\n",
    "url = 'http://gandissect.csail.mit.edu/models/alexnet_places365-6d3c0e75.pth'\n",
    "try:\n",
    "    sd = torch.hub.load_state_dict_from_url(url) # pytorch 1.1\n",
    "except:\n",
    "    sd = torch.hub.model_zoo.load_url(url) # pytorch 1.0\n",
    "sd = sd['state_dict']\n",
    "sd = {k.replace('.module', ''): v for k, v in sd.items()}\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "from netdissect import nethook\n",
    "model = nethook.InstrumentedModel(model)\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "from urllib.request import urlopen\n",
    "\n",
    "synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
    "classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import segmenter\n",
    "segmodel = segmenter.UnifiedParsingSegmenter(segsizes=[256])\n",
    "seglabels = [l for l, c in segmodel.get_label_and_category_names()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from netdissect import parallelfolder\n",
    "from torchvision import transforms\n",
    "\n",
    "reload(parallelfolder)\n",
    "\n",
    "center_crop = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.CenterCrop(227),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = parallelfolder.ParallelImageFolders(\n",
    "    ['dataset/places/val'], transform=[center_crop],\n",
    "    classification=True,\n",
    "    shuffle=True)\n",
    "\n",
    "train_dataset = parallelfolder.ParallelImageFolders(\n",
    "    ['dataset/places/train'], transform=[center_crop],\n",
    "    classification=True,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test classifier on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize\n",
    "\n",
    "indices = range(200,204)\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "truenames = [classlabels[dataset[i][1]] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import show\n",
    "\n",
    "show([[img, pred, tn] for img, pred, tn in zip(imgs, prednames, truenames)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create adapter to segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize\n",
    "reload(renormalize)\n",
    "renorm = renormalize.renormalizer(dataset, mode='zc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segment single image, and visualize the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz, upsample, segviz\n",
    "reload(segviz)\n",
    "\n",
    "iv = imgviz.ImageVisualizer(120, source=dataset, convolutions=model.model.features[:10])\n",
    "            \n",
    "seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
    "\n",
    "show([(iv.image(batch[i]), iv.segmentation(seg[i,0]),\n",
    "            iv.segment_key(seg[i,0], segmodel))\n",
    "            for i in range(len(seg))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize activations for single layer of single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.stop_retaining_layers([('features.8', 'conv5')])\n",
    "model.retain_layer(('features.10', 'conv5'))\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "\n",
    "\n",
    "from matplotlib import cm\n",
    "cm.hot([[0.1, 0.3, 0.4], [0.1, 0.3, 0.4]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "\n",
    "acts = model.retained_layer('conv5').cpu()\n",
    "iv = imgviz.ImageVisualizer((56, 56), source=dataset, convolutions=list(model.model.features.children())[:10])\n",
    "show.blocks(\n",
    "    [[[iv.masked_image(batch[0], acts, (0, u))],\n",
    "      [iv.heatmap(acts, (0, u), mode='nearest')]] for u in range(acts.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally, upsample\n",
    "\n",
    "upfn = upsample.upsampler(\n",
    "    (56, 56),                     # The target output shape\n",
    "    source=dataset,\n",
    "    convolutions=list(model.model.features.children())[:10]  # The conv modules from input to data reveal strides\n",
    ")\n",
    "\n",
    "\n",
    "def compute_segments(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    return seg\n",
    "\n",
    "\n",
    "\n",
    "# TODO: maybe a graph to visualize the results on a small batch like this.\n",
    "# out = compute_conditional_samples([batch.cuda()])\n",
    "# for index, stats in out:\n",
    "#     print(index, stats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "reload(tally)\n",
    "\n",
    "def compute_conditional_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer('conv5')\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    hacts = torch.nn.functional.interpolate(acts, size=seg.shape[2:],\n",
    "                                            mode='bilinear', align_corners=False)\n",
    "    hacts = upfn(acts)\n",
    "    return tally.conditional_samples(hacts, seg)\n",
    "\n",
    "\n",
    "condq = tally.tally_conditional_quantile(compute_conditional_samples,\n",
    "        dataset, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "def compute_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer('conv5')\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "\n",
    "\n",
    "rq = tally.tally_quantile(compute_samples, dataset, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = rq.quantiles(0.99)\n",
    "act[0] = 5\n",
    "rq.normalize(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "condvar = tally.tally_conditional_mean(compute_conditional_samples,\n",
    "        dataset, sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "segbc = tally.tally_bincount(compute_segments,\n",
    "        dataset, sample_size=1000, multi_label_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "for conceptnum, label in enumerate(seglabels):\n",
    "    if conceptnum not in condq.keys():\n",
    "        continue\n",
    "    amt, ind = ((condq.conditional(conceptnum).readout(1000) -\n",
    "      condq.conditional(0).readout(1000)).abs().sum(1)/1000).max(0)\n",
    "    print(conceptnum, amt.item(), 'unit', ind.item(), condq.conditional(conceptnum).size(), label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "def dens(cum):\n",
    "    return cum[1:] - cum[:-1]\n",
    "baseline = condq.conditional(0).readout(1001)[ind].numpy()\n",
    "conditioned = condq.conditional(1).readout(1001)[ind].numpy()\n",
    "top = max(baseline.max(), conditioned.max())\n",
    "buckets = numpy.linspace(0, top, 25)\n",
    "ax.hist(baseline, buckets, alpha=0.5)\n",
    "ax.hist(conditioned, buckets, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize high-activation regions for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "reload(tally)\n",
    "\n",
    "\n",
    "def compute_image_max(batch):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer('conv5')\n",
    "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
    "    acts = acts.max(2)[0]\n",
    "    return acts\n",
    "\n",
    "topk = tally.tally_topk(compute_image_max, dataset, sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "from IPython.display import display\n",
    "\n",
    "reload(imgviz)\n",
    "iv = imgviz.ImageVisualizer((100, 100), source=dataset, convolutions=list(model.model.features.children())[:10])\n",
    "\n",
    "def unit_viz_row(unitnum):\n",
    "    out = []\n",
    "    for imgnum in topk.result()[1][unitnum][:8]:\n",
    "        img = dataset[imgnum][0][None,...].cuda()\n",
    "        scores = model(img.cuda())\n",
    "        pred = classlabels[scores.max(1)[1].item()].split('/')[0]\n",
    "        acts = model.retained_layer('conv5')\n",
    "        out.append([# [iv.image(img[0]), pred],\n",
    "                    # [iv.heatmap(acts, (0, unitnum), mode='nearest'), str(acts[0, unitnum].max().item())[:5]],\n",
    "                    [iv.masked_image(img[0], acts, (0, unitnum)), imgnum.item()],\n",
    "                   ])\n",
    "    return out\n",
    "display(show.blocks(unit_viz_row(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conceptlist = []\n",
    "for conceptnum, label in enumerate(seglabels):\n",
    "    if not condq.has_conditional(conceptnum):\n",
    "        continue\n",
    "    ratio, index = (abs(condq.conditional(conceptnum).mean() - rq.mean()) / rq.mean()).max(0)\n",
    "    stdev = ((condq.conditional(conceptnum).stdev()) / rq.mean())[index]\n",
    "    print(label, 'unit', index.item(), 'ratio', ratio.item(), 'size', condq.conditional(conceptnum).size())\n",
    "    conceptlist.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from netdissect import bargraph\n",
    "reload(bargraph)\n",
    "from IPython.display import display, SVG, HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "def graph_conceptlist(conceptlist):\n",
    "    count = defaultdict(int)\n",
    "    for c in conceptlist:\n",
    "        count[c] += 1\n",
    "    labels, counts = zip(*sorted(count.items(), key=lambda x: -x[1]))\n",
    "    return HTML('<div style=\"height:200px;width:5000px\">' + bargraph.make_svg_bargraph(labels, counts) + '</div>')\n",
    "\n",
    "graph_conceptlist(conceptlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrimination metric.\n",
    "\n",
    "Given only the number of pixels in a given segmentation, how accurately can we do binary classification on a particular scene class?\n",
    "\n",
    "To answer this, we can use conditional quantile information.\n",
    "\n",
    "Conditioned on each scene class, we collect the fraction of pixels in each segmentation class.\n",
    "Then at a given threshold t, the accuracy of scene classification is as follows:\n",
    "p(c | s>t) + p(~c | s < t) = p(c | s > t) + 1 - p(c| s< t)\n",
    "= p(s > t | c) * p(c)/ p(s>t) + 1 -  p(s<t|c) * p(c) / p(s<t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tally)\n",
    "\n",
    "def compute_conditional_discrimination(batch, *args):\n",
    "    assert len(batch) == 1\n",
    "    image_batch = batch.cuda()\n",
    "    scores = model(image_batch)\n",
    "    pred = scores.max(1)[1]\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    feat = seg.view(-1).bincount(minlength=len(seglabels)).float() / (seg.shape[3] * seg.shape[2])\n",
    "    feat[0] = 0\n",
    "    feat = feat[None,...]\n",
    "    return [(0, feat), (pred[0].item() + 1, feat)]\n",
    "\n",
    "conddis = tally.tally_conditional_quantile(compute_conditional_discrimination,\n",
    "        train_dataset, sample_size=50000, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conddis.conditional(0).size()\n",
    "condprob = torch.logspace(-3, 0, 10)[:-1] # p(s>t | c)\n",
    "accscores = torch.zeros((max(conddis.keys()) + 1, 256))\n",
    "\n",
    "c = 1\n",
    "print(classlabels[c])\n",
    "# if c == 0:\n",
    "#     continue\n",
    "level = 1 - conddis.conditional(c).quantiles(condprob)  # Levels at which the conditional quantile is achieved.\n",
    "segprob = conddis.conditional(0).normalize(level)\n",
    "margprob = float(conddis.conditional(c).size()) / conddis.conditional(0).size()\n",
    "acc = condprob * margprob / (segprob) + 1 - (1 - condprob) * margprob / (1 - segprob)\n",
    "acc1 = condprob * margprob / (segprob)\n",
    "acc2 = 1 - (1 - condprob) * margprob / (1 - segprob)\n",
    "#acc = condprob * 0.5 / (condprob  + segprob) # + 1 - (1 - condprob) * margprob / (1 - segprob)\n",
    "# acc = condprob * 0.5 / ((condprob+ segprob)/2) + 1 - (1 - condprob) * 0.5 / (1 - (segprob + condprob)/2)\n",
    "# acc = 1 - (1 - condprob) * 0.5 / (1 - (segprob + condprob)/2)\n",
    "print(acc.shape)\n",
    "\n",
    "s = 1\n",
    "print(seglabels[s])\n",
    "# plt.plot(condprob.numpy(), acc[s].numpy())\n",
    "# plt.plot(condprob.numpy(), acc1[s].numpy())\n",
    "# plt.plot(condprob.numpy(), 1 - acc2[s].numpy())\n",
    "level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1.\n",
    "\n",
    "Assign a label to each unit according to the highest iou at fixed threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "iouscores_at_99 = torch.zeros((max(condq.keys()) + 1, 256))\n",
    "# Compute at fixed quantile\n",
    "actquantile = 0.01\n",
    "actlevel = condq.conditional(0).quantiles([1 - actquantile])[:,0]\n",
    "for c in sorted(condvar.keys()):\n",
    "    if c == 0 or condvar.conditional(c).batchcount <= 1:\n",
    "        continue\n",
    "    levelp = condq.conditional(c).normalize(actlevel)\n",
    "    cp = segbc.mean()[c]\n",
    "    iouscores_at_99[c] = cp * (1 - levelp) / (actquantile + cp * levelp)\n",
    "conceptlist_at_99 = []\n",
    "for u in range(256):\n",
    "    iou, c = iouscores_at_99[:,u].max(0)\n",
    "    if iou.item() < 0.05:\n",
    "        continue\n",
    "    c = c.item()\n",
    "    diff = condvar.conditional(c).mean()[u] - condvar.conditional(0).mean()[u]\n",
    "    display(show.blocks([[seglabels[c],\n",
    "                          'iou %.2f' % iou.item(),\n",
    "                          'dm %.2f' % diff.item(),\n",
    "                          'cnt %d' % condvar.conditional(c).batchcount,\n",
    "                          'unit %d' % u]] + unit_viz_row(u)))\n",
    "    conceptlist_at_99.append(seglabels[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph_conceptlist(conceptlist_at_99))\n",
    "print(str(len(conceptlist_at_99) + ' units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from netdissect import pbar\n",
    "iouscores_at_match = torch.zeros((max(condq.keys()) + 1, 256))\n",
    "# Compute at matching quantile\n",
    "for c in pbar(sorted(condvar.keys())):\n",
    "    if c == 0 or condvar.conditional(c).batchcount <= 1:\n",
    "        continue\n",
    "    cp = float(condq.conditional(c).size()) / condq.conditional(0).size()\n",
    "    actquantile = cp\n",
    "    actlevel = condq.conditional(0).quantiles([1 - actquantile])[:,0]\n",
    "    levelp = condq.conditional(c).normalize(actlevel)\n",
    "    iouscores_at_match[c] = cp * (1 - levelp) / (actquantile + cp * levelp)\n",
    "conceptlist_at_match = []\n",
    "for u in range(256):\n",
    "    iou, c = iouscores_at_match[:,u].max(0)\n",
    "    c = c.item()\n",
    "    diff = condvar.conditional(c).mean()[u] - condvar.conditional(0).mean()[u]\n",
    "    condquantile = float(condq.conditional(c).size()) / condq.conditional(0).size()\n",
    "    display(show.blocks([[seglabels[c],\n",
    "                          'iou %.2f' % iou.item(),\n",
    "                          'dm %.2f' % diff.item(),\n",
    "                          'fq %.2f' % condquantile,\n",
    "                          'cnt %d' % condvar.conditional(c).batchcount,\n",
    "                          'unit %d' % u]] + unit_viz_row(u)))\n",
    "    conceptlist_at_match.append(seglabels[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conceptlist(conceptlist_at_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2.\n",
    "\n",
    "Assign a label to each unit according to the shift in conditional mean with highest statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "zscores = torch.zeros((max(condvar.keys()) + 1, 256))\n",
    "for c in sorted(condvar.keys()):\n",
    "    if c == 0 or condvar.conditional(c).batchcount <= 1:\n",
    "        continue\n",
    "    zscores[c] = ((condvar.conditional(c).mean() - condvar.conditional(0).mean()) /\n",
    "        (condvar.conditional(0).variance() / condvar.conditional(0).batchcount\n",
    "         + condvar.conditional(c).variance() / condvar.conditional(c).batchcount).sqrt())\n",
    "conceptlist_by_zscore = []\n",
    "for u in range(256):\n",
    "    zt, c = zscores[:,u].max(0)\n",
    "    c = c.item()\n",
    "    diff = condvar.conditional(c).mean()[u] - condvar.conditional(0).mean()[u]\n",
    "    display(show.blocks([[seglabels[c],\n",
    "                          'dm %.2f' % diff.item(),\n",
    "                          'zs %.2f' % zt.item(),\n",
    "                          'cnt %d' % condvar.conditional(c).batchcount,\n",
    "                          'unit %d' % u]] + unit_viz_row(u)))\n",
    "    conceptlist_by_zscore.append(seglabels[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conceptlist(conceptlist_by_zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3.\n",
    "\n",
    "Assign a label according to the highest relative mutual information at a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}